{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 4, Model 2A NN: Glaucoma Detection using a neural network on the fundus images of the eye + selected patient details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install scikit-learn\n",
    "%pip install seaborn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import linear_model\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.svm import SVC\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.applications.mobilenet_v3 import preprocess_input \n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, roc_auc_score, f1_score\n",
    "from sklearn.preprocessing import StandardScaler  # Fix import statement\n",
    "# to prevent unnecessary warnings\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "\n",
    "# TensorFlow and tf.keras\n",
    "import tensorflow as tf\n",
    "\n",
    "from pathlib import Path\n",
    "\n",
    "#import useful module for keras library\n",
    "from tensorflow import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Flatten, Conv2D, MaxPooling2D, BatchNormalization\n",
    "from keras.optimizers import Adam\n",
    "from keras.losses import SparseCategoricalCrossentropy\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.regularizers import l2\n",
    "\n",
    "# get modules from sklearn library\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "#import libraries\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import random\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "data = pd.read_csv('metadata - standardized.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# know column type\n",
    "data['fundus_oc_seg'].dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(data['fundus'][12044])\n",
    "print(data['fundus_oc_seg'][12044])\n",
    "print(data['fundus_od_seg'][12044])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def viewFundus(image_path):\n",
    "\n",
    "    image_path = 'full-fundus' + image_path\n",
    "\n",
    "    print(image_path)\n",
    "\n",
    "    # Load the image\n",
    "    image = cv2.imread(image_path)\n",
    "\n",
    "    # Convert the image from BGR to RGB color space\n",
    "    image_rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "    # Display the image\n",
    "    plt.imshow(image_rgb)\n",
    "    plt.axis('on')  # Hide the axis to only show the image\n",
    "    plt.show()\n",
    "\n",
    "viewFundus(data['fundus'][12044])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['eye'] = data['eye'].replace({'OD': 0, 'OS': 1})\n",
    "\n",
    "data[['eye']].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.experimental import enable_iterative_imputer\n",
    "from sklearn.impute import IterativeImputer\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "# Assuming 'data' is your original DataFrame\n",
    "\n",
    "# Separate the data into numerical and non-numerical subsets\n",
    "numerical_data = data.select_dtypes(include=[np.number])  # only numerical features\n",
    "\n",
    "# Check and fill columns with all NaN values\n",
    "for col in numerical_data.columns:\n",
    "    if numerical_data[col].isna().all():\n",
    "        numerical_data[col].fillna(0, inplace=True)  # or use another strategy\n",
    "\n",
    "# Check the number of columns before imputation\n",
    "print(\"Number of columns before imputation:\", numerical_data.shape[1])\n",
    "\n",
    "# Initialize the IterativeImputer with a RandomForestRegressor\n",
    "imp = IterativeImputer(RandomForestRegressor(), max_iter=10, random_state=0)\n",
    "\n",
    "# Perform the imputation - fit and transform the numerical data\n",
    "numerical_data_imputed = imp.fit_transform(numerical_data)\n",
    "\n",
    "# Convert the imputed numerical data back to a pandas DataFrame\n",
    "# Debugging: Check the shape of the imputed data\n",
    "print(\"Shape of imputed data:\", numerical_data_imputed.shape)\n",
    "\n",
    "# Create the DataFrame using the original column names\n",
    "# Here we're assuming that the columns should match the original\n",
    "# If not, adjust the columns parameter accordingly\n",
    "numerical_data_imputed = pd.DataFrame(numerical_data_imputed, columns=numerical_data.columns, index=numerical_data.index)\n",
    "\n",
    "# Combine the imputed numerical data back with the non-numerical data\n",
    "non_numerical_data = data.select_dtypes(exclude=[np.number])  # reselect to include changes\n",
    "data_imputed = pd.concat([numerical_data_imputed, non_numerical_data], axis=1)\n",
    "\n",
    "# Ensure the order of rows remains the same\n",
    "data_imputed = data_imputed.loc[data.index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(data_imputed.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_imputed[[\"eye\"]].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_imputed['eye'] = data_imputed['eye'].round().astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_imputed[[\"eye\"]].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data_imputed\n",
    "\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#removing the patients with no fundus classification and with fundus classification -1 instead of 0,1\n",
    "data = data[\n",
    "    \n",
    "    (data['fundus'].notnull()) & (data['types'] != -1)\n",
    "    \n",
    "]\n",
    "\n",
    "fundus_subset = data[['types', 'fundus', 'names']]\n",
    "\n",
    "fundus_subset['combined'] = fundus_subset['names'] + '.png'\n",
    "\n",
    "fundus_subset['fundus'] = fundus_subset['fundus'].astype(str)\n",
    "\n",
    "fundus_subset['types'] = fundus_subset['types'].astype(str)\n",
    "\n",
    "fundus_subset.info()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fundus_subset['types'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fundus_subset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "\n",
    "# Calculate the number of samples for each class\n",
    "glaucomaCount = len(fundus_subset[fundus_subset['types'] == '1.0'])\n",
    "healthyCount = len(fundus_subset[fundus_subset['types'] == '0.0'])\n",
    "\n",
    "print(\"this is the glaucoma count\", glaucomaCount)\n",
    "print(\"this is the healthy eyes count\", healthyCount)\n",
    "\n",
    "# Set the percentages for the test and validation splits\n",
    "test_percentage = 0.20  # 20% of the data for testing\n",
    "val_percentage_from_train = 0.15  # 25% of the remaining data after test split for validation\n",
    "\n",
    "# Assuming fundus_subset is your DataFrame and 'types' is the column with labels\n",
    "\n",
    "# Set the random seed for reproducibility\n",
    "random_state = 1\n",
    "\n",
    "# Create StratifiedShuffleSplit instance for test set with percentage\n",
    "stratified_split_test = StratifiedShuffleSplit(n_splits=1, test_size=test_percentage, random_state=random_state)\n",
    "\n",
    "for train_index, test_index in stratified_split_test.split(fundus_subset, fundus_subset['types']):\n",
    "    train_set_temp = fundus_subset.iloc[train_index]\n",
    "    test = fundus_subset.iloc[test_index]\n",
    "\n",
    "# Note: Adjusted validation split to calculate percentage from the remaining train set\n",
    "# Create StratifiedShuffleSplit instance for validation set with percentage\n",
    "stratified_split_val = StratifiedShuffleSplit(n_splits=1, test_size=val_percentage_from_train, random_state=random_state)\n",
    "\n",
    "for train_index, val_index in stratified_split_val.split(train_set_temp, train_set_temp['types']):\n",
    "    train = train_set_temp.iloc[train_index]\n",
    "    val = train_set_temp.iloc[val_index]\n",
    "\n",
    "# Print the counts for each set and total\n",
    "print(\"\\nTotal fundus images to be used:\", len(fundus_subset))\n",
    "print(\"\\nTrain set size:\", len(train))\n",
    "print(\"Validation set size:\", len(val))\n",
    "print(\"Test set size:\", len(test))\n",
    "\n",
    "# You can access the features and labels as needed\n",
    "train_features = train.drop('types', axis=1)  # assuming 'types' is the label column\n",
    "train_labels = train['types']\n",
    "val_features = val.drop('types', axis=1)\n",
    "val_labels = val['types']\n",
    "test_features = test.drop('types', axis=1)\n",
    "test_labels = test['types']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Train Size: \", len(train))\n",
    "print(\"Test Size: \", len(test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf; print(tf.config.list_physical_devices('GPU'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from keras.layers.experimental.preprocessing import Rescaling\n",
    "from keras import layers\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.applications.mobilenet_v3 import preprocess_input \n",
    "\n",
    "\n",
    "# Set target size and batch size for data generator\n",
    "target = 224\n",
    "batchSize = 8\n",
    "\n",
    "# Set the class mode to 'binary' for training data generator\n",
    "classes = 'binary'\n",
    "\n",
    "with tf.device('/GPU:0'):\n",
    "\n",
    "\n",
    "    # Create the training data generator\n",
    "    trainDataGen = ImageDataGenerator(preprocessing_function=preprocess_input)\n",
    "    trainGen = trainDataGen.flow_from_dataframe(dataframe=train, \n",
    "                                                directory='full-fundus/full-fundus', \n",
    "                                                class_mode = classes,\n",
    "                                                batch_size = batchSize, \n",
    "                                                shuffle=True, \n",
    "                                                x_col=\"combined\", \n",
    "                                                y_col=\"types\", \n",
    "                                                validate_filenames=True, \n",
    "                                                target_size=(target, target), \n",
    "                                                color_mode='rgb')\n",
    "\n",
    "\n",
    "    # Create the validation data generator\n",
    "    valDataGen = ImageDataGenerator(preprocessing_function=preprocess_input)\n",
    "    valGen = valDataGen.flow_from_dataframe(dataframe=val,\n",
    "                                            directory='full-fundus/full-fundus',   \n",
    "                                            batch_size = batchSize, \n",
    "                                            class_mode = classes,\n",
    "                                            shuffle=False, \n",
    "                                            x_col=\"combined\", \n",
    "                                            y_col=\"types\", \n",
    "                                            validate_filenames=True, \n",
    "                                            target_size=(target, target), \n",
    "                                            color_mode='rgb')\n",
    "\n",
    "\n",
    "    # Create the testing data generator\n",
    "    testDataGen = ImageDataGenerator(preprocessing_function=preprocess_input)\n",
    "    testGen = testDataGen.flow_from_dataframe(dataframe=test,\n",
    "                                            directory='full-fundus/full-fundus',   \n",
    "                                            batch_size = batchSize, \n",
    "                                            class_mode = classes,\n",
    "                                            shuffle=False, \n",
    "                                            x_col=\"combined\", \n",
    "                                            y_col=\"types\", \n",
    "                                            validate_filenames=True, \n",
    "                                            target_size=(target, target), \n",
    "                                            color_mode='rgb')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select the non-image features from your dataframe\n",
    "non_image_features = data[['gender', 'age', 'eye', 'sbp', 'dbp', 'hr', 'iop', 'vcdr', 'cdr_avg']]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
