{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 1, Model 1A: Basic Glaucoma Detection using a support vector machine (SVM) classifier on the fundus images of the eye. The features were extracted using the MobileNetV3 model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install scikit-learn\n",
    "%pip install seaborn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import linear_model\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.svm import SVC\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.applications.mobilenet_v3 import preprocess_input \n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, roc_auc_score, f1_score\n",
    "from sklearn.preprocessing import StandardScaler  # Fix import statement\n",
    "# to prevent unnecessary warnings\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "\n",
    "# TensorFlow and tf.keras\n",
    "import tensorflow as tf\n",
    "\n",
    "from pathlib import Path\n",
    "\n",
    "#import useful module for keras library\n",
    "from tensorflow import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Flatten, Conv2D, MaxPooling2D, BatchNormalization\n",
    "from keras.optimizers import Adam\n",
    "from keras.losses import SparseCategoricalCrossentropy\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.regularizers import l2\n",
    "\n",
    "# get modules from sklearn library\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "#import libraries\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import random\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "data = pd.read_csv('metadata - standardized.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[data['fundus_oc_seg'].notnull() & data['fundus_od_seg'].notnull()].info()\n",
    "\n",
    "print(data['fundus_oc_seg'].notnull().value_counts())\n",
    "\n",
    "data[data['fundus_oc_seg'].notnull()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# know column type\n",
    "data['fundus_oc_seg'].dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['fundus_oc_seg'][12044]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['fundus_od_seg'][12044]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(data['fundus'][12044])\n",
    "print(data['fundus_oc_seg'][12044])\n",
    "print(data['fundus_od_seg'][12044])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "\n",
    "def viewFundus(image_path):\n",
    "\n",
    "    image_path = 'full-fundus' + image_path\n",
    "\n",
    "    print(image_path)\n",
    "\n",
    "    # Load the image\n",
    "    image = cv2.imread(image_path)\n",
    "\n",
    "    # Convert the image from BGR to RGB color space\n",
    "    image_rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "    # Display the image\n",
    "    plt.imshow(image_rgb)\n",
    "    plt.axis('on')  # Hide the axis to only show the image\n",
    "    plt.show()\n",
    "\n",
    "def viewOpticCup(image_path):\n",
    "\n",
    "    image_path = 'optic-cup' + image_path\n",
    "\n",
    "    print(image_path)\n",
    "\n",
    "    # Load the image\n",
    "    image = cv2.imread(image_path)\n",
    "\n",
    "    # Convert the image from BGR to RGB color space\n",
    "    image_rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "    # Display the image\n",
    "    plt.imshow(image_rgb)\n",
    "    plt.axis('on')  # Hide the axis to only show the image\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "viewFundus(data['fundus'][12044])\n",
    "viewOpticCup(data['fundus_oc_seg'][12044])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#removing the patients with no fundus classification and with fundus classification -1 instead of 0,1\n",
    "fundus_subset = data[(data['fundus'].notnull()) & (data['types'] != -1)]\n",
    "\n",
    "fundus_subset = fundus_subset[['types', 'fundus', 'names']]\n",
    "\n",
    "fundus_subset['combined'] = fundus_subset['names'] + '.png'\n",
    "\n",
    "fundus_subset['fundus'] = fundus_subset['fundus'].astype(str)\n",
    "\n",
    "fundus_subset['types'] = fundus_subset['types'].astype(str)\n",
    "\n",
    "fundus_subset.info()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fundus_subset['types'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fundus_subset = data[(data['fundus'].notnull()) & (data['fundus'] != -1)]\n",
    "fundus_subset = fundus_subset[['types', 'fundus', 'names']]\n",
    "\n",
    "fundus_subset['combined'] = fundus_subset['names'] + '.png'\n",
    "\n",
    "fundus_subset['fundus'] = fundus_subset['fundus'].astype(str)\n",
    "\n",
    "fundus_subset['types'] = fundus_subset['types'].astype(str)\n",
    "\n",
    "fundus_subset.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fundus_subset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# glaucoma fundus images\n",
    "glaucomaCount = len(fundus_subset[(fundus_subset['types'] == '1')])\n",
    "print(\"this is the glaucoma count\", glaucomaCount)\n",
    "\n",
    "# healthy fundus images\n",
    "healthyCount = len(fundus_subset[(fundus_subset['types'] == '0')])\n",
    "print(\"this is the healthy eyes count\", healthyCount)\n",
    "\n",
    "print('\\nTotal fundus images to  be used: ', glaucomaCount + healthyCount)\n",
    "\n",
    "# randomize data\n",
    "fundus_subset_random = fundus_subset.sample(frac=1, random_state=1)\n",
    "\n",
    "# create df for each class based on randomized data\n",
    "healthy_subset = fundus_subset_random[fundus_subset_random['types'] == '0']\n",
    "glaucoma_subset = fundus_subset_random[fundus_subset_random['types'] == '1']\n",
    "\n",
    "testSize = 500\n",
    "valSize = 500\n",
    "\n",
    "# get train items\n",
    "healthy_train = healthy_subset.head(healthyCount - testSize)\n",
    "glaucoma_train = glaucoma_subset.head(glaucomaCount - testSize)\n",
    "train = healthy_train._append(glaucoma_train)\n",
    "\n",
    "# get val items\n",
    "healthy_val = healthy_subset.tail(valSize)\n",
    "glaucoma_val = glaucoma_subset.tail(valSize)\n",
    "val = healthy_val._append(glaucoma_val)\n",
    "\n",
    "# get test items\n",
    "healthy_test = healthy_subset.tail(testSize)\n",
    "glaucoma_test = glaucoma_subset.tail(testSize)\n",
    "test = healthy_test._append(glaucoma_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Train Size: \", len(train))\n",
    "print(\"Test Size: \", len(test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf; print(tf.config.list_physical_devices('GPU'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from keras.layers.experimental.preprocessing import Rescaling\n",
    "from keras import layers\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.applications.mobilenet_v3 import preprocess_input \n",
    "\n",
    "\n",
    "# Set target size and batch size for data generator\n",
    "target = 224\n",
    "batchSize = 8\n",
    "\n",
    "# Set the class mode to 'binary' for training data generator\n",
    "classes = 'binary'\n",
    "\n",
    "with tf.device('/GPU:0'):\n",
    "\n",
    "\n",
    "    # Create the training data generator\n",
    "    trainDataGen = ImageDataGenerator(preprocessing_function=preprocess_input)\n",
    "    trainGen = trainDataGen.flow_from_dataframe(dataframe=train, \n",
    "                                                directory='full-fundus/full-fundus', \n",
    "                                                class_mode = classes,\n",
    "                                                batch_size = batchSize, \n",
    "                                                shuffle=True, \n",
    "                                                x_col=\"combined\", \n",
    "                                                y_col=\"types\", \n",
    "                                                validate_filenames=True, \n",
    "                                                target_size=(target, target), \n",
    "                                                color_mode='rgb')\n",
    "\n",
    "\n",
    "    # Create the validation data generator\n",
    "    valDataGen = ImageDataGenerator(preprocessing_function=preprocess_input)\n",
    "    valGen = valDataGen.flow_from_dataframe(dataframe=val,\n",
    "                                            directory='full-fundus/full-fundus',   \n",
    "                                            batch_size = batchSize, \n",
    "                                            class_mode = classes,\n",
    "                                            shuffle=False, \n",
    "                                            x_col=\"combined\", \n",
    "                                            y_col=\"types\", \n",
    "                                            validate_filenames=True, \n",
    "                                            target_size=(target, target), \n",
    "                                            color_mode='rgb')\n",
    "\n",
    "\n",
    "    # Create the testing data generator\n",
    "    testDataGen = ImageDataGenerator(preprocessing_function=preprocess_input)\n",
    "    testGen = testDataGen.flow_from_dataframe(dataframe=test,\n",
    "                                            directory='full-fundus/full-fundus',   \n",
    "                                            batch_size = batchSize, \n",
    "                                            class_mode = classes,\n",
    "                                            shuffle=False, \n",
    "                                            x_col=\"combined\", \n",
    "                                            y_col=\"types\", \n",
    "                                            validate_filenames=True, \n",
    "                                            target_size=(target, target), \n",
    "                                            color_mode='rgb')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.device('/GPU:0'):\n",
    "\n",
    "    # Set class mode to 'sparse' for visualization data generators\n",
    "    classes = 'sparse'\n",
    "\n",
    "    # Create data generators for healthy and glaucoma subsets for visualization\n",
    "    healthy_subsetDataGen = ImageDataGenerator(preprocessing_function=preprocess_input)\n",
    "    healthy_subset_Gen = trainDataGen.flow_from_dataframe(dataframe=healthy_subset, \n",
    "                                                directory='full-fundus/full-fundus', \n",
    "                                                class_mode = classes,\n",
    "                                                batch_size = batchSize, \n",
    "                                                shuffle=True, \n",
    "                                                x_col=\"combined\", \n",
    "                                                y_col=\"types\", \n",
    "                                                validate_filenames=True, \n",
    "                                                target_size=(target, target), \n",
    "                                                color_mode='rgb')\n",
    "\n",
    "\n",
    "    glaucoma_subsetDataGen = ImageDataGenerator(preprocessing_function=preprocess_input)\n",
    "    glaucoma_subset_Gen = glaucoma_subsetDataGen.flow_from_dataframe(dataframe=glaucoma_subset, \n",
    "                                                directory='full-fundus/full-fundus', \n",
    "                                                class_mode = classes,\n",
    "                                                batch_size = batchSize, \n",
    "                                                shuffle=True, \n",
    "                                                x_col=\"combined\", \n",
    "                                                y_col=\"types\", \n",
    "                                                validate_filenames=True, \n",
    "                                                target_size=(target, target), \n",
    "                                                color_mode='rgb')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_images(gen):\n",
    "    \n",
    "    # return classes , images to be displayed\n",
    "    g_dict = gen.class_indices        # defines dictionary {'class': index}\n",
    "    classes = list(g_dict.keys())     # defines list of dictionary's kays (classes), classes names : string\n",
    "    images, labels = next(gen)        # get a batch size samples from the generator\n",
    "\n",
    "    # calculate number of displayed samples\n",
    "    length = len(labels)        # length of batch size\n",
    "    sample = min(length, 25)    # check if sample less than 25 images\n",
    "\n",
    "    plt.figure(figsize= (20, 20))\n",
    "\n",
    "    for i in range(sample):\n",
    "        plt.subplot(4, 4, i + 1)\n",
    "        image = images[i] / 255       # scales data to range (0 - 255)\n",
    "        plt.imshow(image)\n",
    "        index = np.argmax(labels[i])  # get image index\n",
    "        class_name = classes[index]   # get class of image\n",
    "        plt.title(class_name, color= 'blue', fontsize= 12)\n",
    "        plt.axis('off')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_images(healthy_subset_Gen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_images(glaucoma_subset_Gen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.applications import MobileNetV3Small\n",
    "from keras import Sequential\n",
    "\n",
    "# Load pre-trained MobileNetV3 model\n",
    "base_model = MobileNetV3Small(weights='imagenet', include_top=False, input_shape=(target, target, 3))\n",
    "\n",
    "model = Sequential([\n",
    "    base_model, layers.GlobalAveragePooling2D()\n",
    "])\n",
    "\n",
    "# Function to extract features using this model\n",
    "def extract_features(generator, sample_count):\n",
    "    features = np.zeros(shape=(sample_count, model.layers[-1].output.shape[1]))\n",
    "    labels = np.zeros(shape=(sample_count))\n",
    "    i = 0\n",
    "    for inputs_batch, labels_batch in generator:\n",
    "        features_batch = model.predict(inputs_batch)\n",
    "        features[i * batchSize: (i + 1) * batchSize] = features_batch\n",
    "        labels[i * batchSize: (i + 1) * batchSize] = labels_batch\n",
    "        i += 1\n",
    "        if i * batchSize >= sample_count:\n",
    "            break\n",
    "    return features, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "train_features, train_labels = extract_features(trainGen, trainGen.samples)\n",
    "val_features, val_labels = extract_features(valGen, valGen.samples)\n",
    "test_features, test_labels = extract_features(testGen, testGen.samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "\n",
    "with tf.device('/GPU:0'):\n",
    "\n",
    "    # Create and train the SVM\n",
    "    svm_model = SVC(kernel='linear')\n",
    "    svm_model.fit(train_features, train_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validate the SVM\n",
    "val_accuracy = svm_model.score(val_features, val_labels)\n",
    "print('Validation accuracy:', val_accuracy)\n",
    "\n",
    "# Test the SVM\n",
    "test_accuracy = svm_model.score(test_features, test_labels)\n",
    "print('Test accuracy:', test_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "\n",
    "test_predictions = svm_model.predict(test_features)\n",
    "\n",
    "conf_matrix = confusion_matrix(test_labels, test_predictions)\n",
    "class_report = classification_report(test_labels, test_predictions)\n",
    "\n",
    "# Print the confusion matrix\n",
    "print(\"Confusion Matrix:\")\n",
    "print(conf_matrix)\n",
    "\n",
    "# Print the classification report\n",
    "print(\"\\nClassification Report:\")\n",
    "print(class_report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the confusion matrix heatmap\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(conf_matrix, annot=True, fmt=\"d\", cmap=\"Blues\", cbar=False)\n",
    "plt.title('Confusion Matrix Heatmap')\n",
    "plt.xlabel('Predicted Labels')\n",
    "plt.ylabel('True Labels')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from keras.models import Sequential\n",
    "# from keras.layers import Dense, Dropout, Flatten, Conv2D, MaxPooling2D, BatchNormalization\n",
    "\n",
    "# # define sequential model\n",
    "# svm_model = Sequential()\n",
    "# # define conv-pool layers - set 1\n",
    "# svm_model.add(Conv2D(filters = 32, kernel_size=(3, 3), strides=(1, 1),\n",
    "# activation='relu', padding='valid', input_shape = (224, 224, 3)))\n",
    "# svm_model.add(BatchNormalization(axis=3))\n",
    "# svm_model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "\n",
    "# # define conv-pool layers - set 2\n",
    "# svm_model.add(Conv2D(filters = 16, kernel_size=(3, 3), strides=(1, 1),\n",
    "# activation='relu', padding='valid'))\n",
    "\n",
    "# svm_model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "# # define conv-pool layers - set 3\n",
    "# svm_model.add(Conv2D(filters = 16, kernel_size=(3, 3), strides=(1, 1),\n",
    "# activation='relu', padding='valid'))\n",
    "# svm_model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "\n",
    "# # add flatten layer\n",
    "# svm_model.add(Flatten())\n",
    "\n",
    "# # add dense layers with some dropout\n",
    "# svm_model.add(Dense(512, activation='relu'))\n",
    "# svm_model.add(Dropout(rate = 0.2))\n",
    "# svm_model.add(Dense(256, activation='relu'))\n",
    "# svm_model.add(Dense(256, activation='relu'))\n",
    "# svm_model.add(Dense(128, activation='relu'))\n",
    "\n",
    "# # add output layer\n",
    "# svm_model.add(Dense(1, kernel_regularizer = l2(0.01), activation='linear'))\n",
    "\n",
    "# svm_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from keras.callbacks import TensorBoard, EarlyStopping\n",
    "# from datetime import datetime\n",
    "\n",
    "# logdir = \"logs/\" + datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "# tensorboard_callback = TensorBoard(log_dir=logdir)\n",
    "\n",
    "# es = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from keras.backend import epsilon\n",
    "\n",
    "# def f1_score_metric(y_true, y_pred):\n",
    "#     y_pred = tf.round(y_pred)\n",
    "#     return 2 * tf.reduce_sum(y_true * y_pred) / (tf.reduce_sum(y_true) + tf.reduce_sum(y_pred) + epsilon())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with tf.device('/GPU:0'):\n",
    "\n",
    "#     svm_model.compile(optimizer = 'adam',\n",
    "#                 loss = 'squared_hinge',\n",
    "#                 metrics=['accuracy','Precision','Recall','AUC',f1_score_metric])\n",
    "\n",
    "#     r=svm_model.fit(trainGen,\n",
    "#                 batch_size = 32,\n",
    "#                 verbose = 1,\n",
    "#                 epochs = 10,\n",
    "#                 validation_data = valGen,\n",
    "#                 callbacks = [tensorboard_callback, es],)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Evaluate the model on the validation data\n",
    "# score=svm_model.evaluate(valGen)\n",
    "# score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Plot the training and validation accuracy and loss\n",
    "# import seaborn as sns\n",
    "# sns.set()\n",
    "\n",
    "# acc = r.history['accuracy']\n",
    "# val_acc = r.history['val_accuracy']\n",
    "# loss = r.history['loss']\n",
    "# val_loss = r.history['val_loss']\n",
    "# epochs = range(1, len(loss) + 1)\n",
    "\n",
    "# #accuracy plot\n",
    "# plt.plot(epochs, acc, color='green', label='Training Accuracy')\n",
    "# plt.plot(epochs, val_acc, color='blue', label='Validation Accuracy')\n",
    "# plt.title('Training and Validation Accuracy')\n",
    "# plt.ylabel('Accuracy')\n",
    "# plt.xlabel('Epoch')\n",
    "# plt.legend()\n",
    "\n",
    "# plt.figure()\n",
    "# #loss plot\n",
    "# plt.plot(epochs, loss, color='green', label='Training Loss')\n",
    "# plt.plot(epochs, val_loss, color='red', label='Validation Loss')\n",
    "# plt.title('Training and Validation Loss')\n",
    "# plt.xlabel('Epoch')\n",
    "# plt.ylabel('Loss')\n",
    "# plt.legend()\n",
    "\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
    "\n",
    "# # Generate predictions\n",
    "# predictions = svm_model.predict(testGen)\n",
    "# predictions = np.round(predictions).astype(int).flatten()  # Adjust based on your output format\n",
    "\n",
    "# # Get true labels\n",
    "# true_labels = testGen.labels\n",
    "\n",
    "# # Calculate accuracy\n",
    "# accuracy = accuracy_score(true_labels, predictions)\n",
    "# print(f\"Accuracy: {accuracy}\")\n",
    "\n",
    "# # Generate and print confusion matrix\n",
    "# conf_matrix = confusion_matrix(true_labels, predictions)\n",
    "# print(\"Confusion Matrix:\")\n",
    "# print(conf_matrix)\n",
    "\n",
    "# # Generate and print classification report\n",
    "# class_report = classification_report(true_labels, predictions)\n",
    "# print(\"Classification Report:\")\n",
    "# print(class_report)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
